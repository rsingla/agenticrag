{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --quiet -U langchain-community langchain-ollama langchain-core tiktoken langchain-nomic \"nomic[local]\"\n",
    "pip install scikit-learn \"nomic[local]\" pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(env_var):\n",
    "    os.environ[env_var] = os.getenv(env_var)\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_TRACING_V2\")\n",
    "_set_env(\"LANGCHAIN_ENDPOINT\")\n",
    "_set_env(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "local_llm = 'llama3.2:3b-instruct-fp16'\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0.0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0.0, format=\"json\")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"{input}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run both the command locally\n",
    "\n",
    "ollama pull llama3.2:3b-instruct-fp16\n",
    "ollama run llama3.2:3b-instruct-fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "loader = PDFMinerLoader(\"data/Algomasterio_System_Design_Interview_Handbook.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\")\n",
    "\n",
    "vector_store = SKLearnVectorStore.from_documents(docs, embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever(k=3)\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "\n",
    "docs = vector_store.similarity_search(query)\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What is the best way to prepare for a system design interview?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = retriever.invoke(\"Tell me all the topics this books covers?\")\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What are the best practices for designing scalable systems?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever.invoke(\"What are the best practices for designing distributed systems?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Router\n",
    "\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "#Prompt\n",
    "router_instructions = \"\"\" You are an expert at routing a user question to Vectorstore to web search.\n",
    "The vector store contains a handbook on system design interview.\n",
    "The web search contains the most recent information on the internet.\n",
    "You will be given a user query and you need to decide which tool to use.\n",
    "You can either choose to search the web or search the vector store.\n",
    "You can only choose one of the options.\n",
    "If you can answer the user question, return \"I can answer that!\"\n",
    "If not, you need to find the most relevant information in the vector store or decide to search the web.\n",
    "return the json with the following schema:\n",
    "{\n",
    "    \"choice\": \"vector_store\" or \"web_search\",\n",
    "    \"reason\": \"reason for choosing the tool\"\n",
    "    \"search_query\": \"search query to use\"\n",
    "    \"search_k\": \"number of results to return\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "question = [HumanMessage(content=\"What is the best way to prepare for a system design interview?\")]\n",
    "test_vector_store = llm_json_mode.invoke([SystemMessage(content=router_instructions)]+ question)\n",
    "json.loads(test_vector_store.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
